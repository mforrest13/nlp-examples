{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1XUz3f4F9-4"
   },
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LcY1sleuaVR_",
    "outputId": "d3938164-8442-44a5-808f-0ef75848dd0d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[1;32m      4\u001b[0m my_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello there. Goodbye everybody.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "my_message = \"Hello there. Goodbye everybody.\"\n",
    "tokens = sent_tokenize(my_message)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjUz7ar8l1Ez",
    "outputId": "4c1a257b-d90c-4b7a-d1a8-3ad1a568c2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['@', 'Everybody', ':', 'Hello', 'NLP-world', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "my_message = \"@Everybody: Hello NLP-world!\"\n",
    "tokens = word_tokenize(my_message)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "693oQ6X1nUFX",
    "outputId": "332eac5e-3518-48ed-dbb9-d746a9c840c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'Everybody', ':', 'Hello', 'NLP', '-', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "tokens = wordpunct_tokenize(my_message)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxS7HjZun4kh",
    "outputId": "b6cf1f8e-2356-4b62-874d-86aa89712157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Everybody', 'Hello', 'NLP', 'world']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "tokens = regexp_tokenize(my_message, r\"\\w+\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ota288o3fDI",
    "outputId": "6a897cb9-bde4-4acb-d8ea-e630f148de38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'this', 'is', 'only', 'an', 'example']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "input_sentences = [\"Hello world\", \"this is only an example\"]\n",
    "tokens = []\n",
    "for word in input_sentences:\n",
    "   tokens.extend(regexp_tokenize(word, r\"\\w+\"))\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmUC3n1rpbAZ"
   },
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3YE64x4spih6",
    "outputId": "309fc462-a775-468f-ebaa-e57e83becec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'enjoy', 'enjoy', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokens = [\"Enjoy\", \"enjoying\", \"enjoys\", \"enjoyable\"]\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_Mdv8wEtEB5",
    "outputId": "4e7f507c-75a9-4a61-827c-55d6e0d8d1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoy', 'enjoy', 'enjoy', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stems = [stemmer.stem(token) for token in tokens]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD2hCfsdtd0g"
   },
   "source": [
    "List-comprehension in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TokbWarcvADn",
    "outputId": "8347f3d7-c5c5-4f3a-d4a6-aadeca3b5437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'white', 'purple', 'yellow', 'blue', 'green', 'black']\n",
      "['red', 'white', 'purple', 'yellow', 'blue', 'green', 'black']\n",
      "['purple', 'blue']\n",
      "['purple', 'blue']\n"
     ]
    }
   ],
   "source": [
    "input_list = [\"red\", \"white\", \"purple\", \"yellow\", \"blue\", \"green\", \"black\"]\n",
    "\n",
    "# Example non-list comprehesion\n",
    "output_list = []\n",
    "for item in input_list:\n",
    "    output_list.append(item)\n",
    "print(output_list)\n",
    "\n",
    "# Example of list-comprehension\n",
    "output_list = [item for item in input_list]\n",
    "print(output_list)\n",
    "\n",
    "# Example non-list comprehesion (with conditional)\n",
    "output_list = []\n",
    "for item in input_list:\n",
    "    if \"u\" in item:\n",
    "      output_list.append(item)\n",
    "print(output_list)\n",
    "\n",
    "# Example of list-comprehension (with conditional)\n",
    "output_list = [item for item in input_list if \"u\" in item]\n",
    "print(output_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSjbRREXq0fW"
   },
   "source": [
    "**Removing stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ju-QZPcYq205",
    "outputId": "ab808da4-5d15-4cfa-9fe5-3b1ba032d1eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "This is an example sentence to test stopwords\n",
      "['This', 'example', 'sentence', 'test', 'stopwords']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "example_text = \"This is an example sentence to test stopwords\"\n",
    "sw_en = stopwords.words(\"english\")\n",
    "\n",
    "text_no_stopwords = [word for word in example_text.split() if word not in sw_en]\n",
    "print(example_text)\n",
    "print(text_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5dCifvIsHBv",
    "outputId": "dacd00e9-1f5a-422d-badd-a5a67fd87676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "The percentage of words without stopwords in Hamlet is 0.013383297644539615 %\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "nltk.download(\"gutenberg\")\n",
    "\n",
    "words = gutenberg.words(\"shakespeare-hamlet.txt\")\n",
    "words_no_stopwords = [word for word in words if word not in sw_en]\n",
    "\n",
    "stopwords_percentage = len(text_no_stopwords) * 100 / len(words)\n",
    "print(\"The percentage of words without stopwords in Hamlet is\", stopwords_percentage, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPM-J7jCWayU"
   },
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cs0oJ25Az2Bt",
    "outputId": "7a767f49-a2cd-4ec5-847a-0c5603975889"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-PRON-', 'be', 'put', 'in', 'effort', 'to', 'enhance', '-PRON-', 'understanding', 'of', 'lemmatization']\n",
      "['We', 'be', 'put', 'in', 'effort', 'to', 'enhance', 'our', 'understanding', 'of', 'lemmatization']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "\n",
    "lemmas = [token.lemma_ for token in nlp(sentence)]\n",
    "print(lemmas)\n",
    "\n",
    "lemmas = [w.lemma_ if w.lemma_ !='-PRON-' else w.text for w in nlp(sentence)]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuT4ZTZgQ-Nf"
   },
   "source": [
    "**POS tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7uji_APRABp",
    "outputId": "d6b37b7a-d043-4870-c491-828aca02fb66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "tokens ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
      "pos_tagged [('We', 'PRP'), ('are', 'VBP'), ('putting', 'VBG'), ('in', 'IN'), ('efforts', 'NNS'), ('to', 'TO'), ('enhance', 'VB'), ('our', 'PRP$'), ('understanding', 'NN'), ('of', 'IN'), ('Lemmatization', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "sentence = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tagged = pos_tag(tokens)\n",
    "\n",
    "print(\"tokens\", tokens)\n",
    "print(\"pos_tagged\", pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dI-WRC0S5WS",
    "outputId": "cc8539c0-3d7f-44a9-e056-e15c9d61e853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "lemmas ['We', 'be', 'put', 'in', 'effort', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Return tag compliance to WordNet lemmatization (a, n, r, v) \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN # Nouns by default\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag)) for word, tag in pos_tagged]\n",
    "\n",
    "print(\"lemmas\", lemmas)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4_text_preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
